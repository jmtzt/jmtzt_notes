{
  
    
        "post0": {
            "title": "Dealing with unknown labels in fastai",
            "content": "Imports . . This article is also a Jupyter Notebook available to be run from the top down. There will be code snippets that you can then run in any environment. . Below are the versions of fastai, fastcore, and wwf currently running at the time of writing this: . fastai : 2.2.5 | fastcore : 1.3.12 | wwf : 0.0.16 | . . from fastai.vision.all import * . Below you will find the exact imports for everything we use today . from functools import partial from fastcore.transform import Pipeline from fastai.callback.fp16 import to_fp16 from fastai.callback.progress import ProgressCallback from fastai.callback.schedule import fine_tune from fastai.data.block import DataBlock, MultiCategoryBlock from fastai.data.external import untar_data, URLs from fastai.data.transforms import RandomSplitter, RegexLabeller, Normalize from fastai.metrics import accuracy_multi, BCEWithLogitsLossFlat from fastai.vision.augment import RandomResizedCrop, aug_transforms from fastai.vision.core import get_image_files, PILImage from fastai.vision.data import ImageBlock, imagenet_stats from fastai.vision.learner import cnn_learner from torchvision.models.resnet import resnet34 . In this notebook, we will use MultiCategoryBlock in a specially clever way to have our model return no labels when the given example does not belong to any of the classes seen during training. So let&#39;s repurpose our previous code for the dataset pets to be able to tell if an image does not belong to any of the breeds seen during training, e.g. a donkey picture. . Getting our Data . path = untar_data(URLs.PETS)/&#39;images&#39; . path.ls()[:3] . (#3) [Path(&#39;/home/jmtzt/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_42.jpg&#39;),Path(&#39;/home/jmtzt/.fastai/data/oxford-iiit-pet/images/miniature_pinscher_105.jpg&#39;),Path(&#39;/home/jmtzt/.fastai/data/oxford-iiit-pet/images/Russian_Blue_179.jpg&#39;)] . We&#39;ll go ahead and make the dataloaders with one important change, the get_y function since MutiCategoryBlock expects a list of labels. . Now we pass that into our get_y along with any labeller . pets_multi = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=Pipeline([RegexLabeller(pat = r&#39;/([^/]+)_ d+.jpg$&#39;), lambda label: [label]]), item_tfms=RandomResizedCrop(460, min_scale=0.75), batch_tfms=[*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats)]) . dls = pets_multi.dataloaders(untar_data(URLs.PETS)/&quot;images&quot;, bs=32) . dls.show_batch(max_n=9, figsize=(7,8)) . Training our Model . Note below a very important detail which is that there are two different thresholds. The metric will use a very high threshold so that only highly confident predictions are accepted as correct. The loss function, however, uses the default 0.5 threshold so that the model is not incentivated to make extreme predictions even if unsure. . learn = cnn_learner(dls, resnet34, pretrained=True, metrics=[partial(accuracy_multi, thresh=0.95)], loss_func=BCEWithLogitsLossFlat(thresh=0.5)).to_fp16() . learn.fine_tune(epochs=4, base_lr=2e-3) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.413433 | 0.065878 | 0.973924 | 00:18 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.053780 | 0.023888 | 0.981312 | 00:23 | . 1 | 0.026235 | 0.013355 | 0.989193 | 00:22 | . 2 | 0.015184 | 0.010225 | 0.991844 | 00:23 | . 3 | 0.010578 | 0.009411 | 0.992539 | 00:23 | . learn.save(&#39;cats-vs-dogs&#39;) . Path(&#39;models/cats-vs-dogs.pth&#39;) . learn.recorder.plot_loss() . That looks very nice..! Let&#39;s see how we did in the next section. . Model evaluation . learn.loss_func=BCEWithLogitsLossFlat(thresh=0.95) . learn.show_results() . img = PILImage.create(&#39;persian_cat.jpg&#39;) img.show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fba3c0b01d0&gt; . learn.predict(img)[0] . (#1) [&#39;Persian&#39;] . Awesome! The model returns only one label and it is the correct one. Let&#39;s see if we try with a donkey picture... . img = PILImage.create(&#39;donkey.jpg&#39;) img.show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fba35b0d9b0&gt; . learn.predict(img)[0] . (#0) [] . Nothing! Our classifier is smart enough to return no label if the picture does not belong to any of the classes seen during training. Isn&#39;t it great? . img = PILImage.create(&#39;real-ai.jpg&#39;) img.show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fba35afd438&gt; . learn.predict(img)[0] . (#0) [] .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastai/unknown/classification/labels/2021/08/09/Unknown-Labels.html",
            "relUrl": "/fastai/unknown/classification/labels/2021/08/09/Unknown-Labels.html",
            "date": " • Aug 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Multi Label Classification with fastai",
            "content": "Imports . from fastai.vision.all import * import pandas as pd import torch from torch import nn from fastcore.meta import use_kwargs_dict from fastai.callback.fp16 import to_fp16 from fastai.callback.progress import ProgressCallback from fastai.callback.schedule import lr_find, fit_one_cycle from fastai.data.block import MultiCategoryBlock, DataBlock from fastai.data.external import untar_data, URLs from fastai.data.transforms import RandomSplitter, ColReader from fastai.metrics import accuracy_multi, BaseLoss from fastai.vision.augment import aug_transforms from fastai.vision.data import ImageBlock from fastai.vision.learner import cnn_learner from torchvision.models import resnet34 . Getting the data . For this multi-label problem, we will use the Planet dataset, where it&#39;s a collection of satellite images with multiple labels describing the scene. I&#39;ll go through and explain a few different ways to make this dataset, highlighting some of the flexibility the new DataBlock API can do. . First, let&#39;s grab our data . planet_source = untar_data(URLs.PLANET_SAMPLE) df = pd.read_csv(planet_source/&#39;labels.csv&#39;) . Now let&#39;s look at how it&#39;s stored. Our DataFrame is formatted so our images filename is the first column, and the labels in the second . df.head() . image_name tags . 0 train_21983 | partly_cloudy primary | . 1 train_9516 | clear cultivation primary water | . 2 train_12664 | haze primary | . 3 train_36960 | clear primary | . 4 train_5302 | haze primary road | . Note: due to some class imbalance issues, there is only one instance of the tag blow_down. As a result we will drop it since there is just one and oversampling would not make sense in this case: . df = df[df[&#39;tags&#39;] != &#39;blow_down clear primary road&#39;] . First Method -&gt; DataBlock . batch_tfms = aug_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.) . planet = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x=ColReader(0, pref=f&#39;{planet_source}/train/&#39;, suff=&#39;.jpg&#39;), splitter=RandomSplitter(), get_y=ColReader(1, label_delim=&#39; &#39;), batch_tfms = batch_tfms) . Multi-label so we want a MultiCategoryBlock | get_x and get_y define how we expect to grab our data | ColReader works with Pandas DataFrames | . Now we can dataloaders by passing in our source folder . dls = planet.dataloaders(df) . dls.show_batch(max_n=9, figsize=(12,9)) . Second Method -&gt; Lambda Functions . This next version will instead use lambda functions to grab our image names, which get&#39;s rid of the ColReader, if you&#39;re more familiar with these . blocks = (ImageBlock, MultiCategoryBlock) . First let&#39;s try our get_x. Our lambda function needs to return a Path() to our particular image. This can be done by including f&#39;{x[0]}.jpg&#39; . But what is that even doing? Let&#39;s take a look . get_x = lambda x:planet_source/&#39;train&#39;/f&#39;{x[0]}.jpg&#39; . If we pass in one row of our DataFrame, we should expect to see the entire path laid out in front of us! . val = df.values[0]; val . array([&#39;train_21983&#39;, &#39;partly_cloudy primary&#39;], dtype=object) . get_x(df.values[0]) . Path(&#39;/home/jmtzt/.fastai/data/planet_sample/train/train_21983.jpg&#39;) . Which it does! A nice, simple, and clean way to grab our paths. Let&#39;s see how our y getter will look like . get_y = lambda x:x[1].split(&#39; &#39;) . Looks fairly close to the previous version, if you pay attention. Remember that our x is the DataFrame&#39;s values, so if we grab position 1 from earlier, we can see that it&#39;s our labels! . Let&#39;s make our full PipeLine now that we&#39;re sure everything will work . planet = DataBlock(blocks=blocks, get_x=get_x, splitter=RandomSplitter(), get_y=get_y, batch_tfms=batch_tfms) . dls = planet.dataloaders(df) dls.show_batch(max_n=9, figsize=(12,9)) . Third Method -&gt; Custom get_items Functions . That previous one worked fine, but shouldn&#39;t I be able to do a one-liner? Since it&#39;s all right there instead of defining our get_x and get_y? There IS! We can create our own function, where we should expect to return both an x and a y value. Let&#39;s make one . def _planet_items(x): return ( f&#39;{planet_source}/train/&#39;+x.image_name+&#39;.jpg&#39;, x.tags.str.split()) . Our DataBlock now looks like so: . planet = DataBlock.from_columns(blocks=(ImageBlock, MultiCategoryBlock), get_items = _planet_items, splitter=RandomSplitter(), batch_tfms=batch_tfms) . That&#39;s all our DataBlock needs if we can plan accordingly. Looks pretty clean! . dls = planet.dataloaders(df) dls.show_batch(max_n=9, figsize=(12,9)) . Training a model . We&#39;ll use a resnet34 for this task and the accuracy_multi metric . from torchvision.models import resnet34 from fastai.metrics import accuracy_multi . learn = cnn_learner(dls, resnet34, pretrained=True, metrics=[accuracy_multi]) . Model results are on a scale with a threshold instead of exact, allows for &quot;it&#39;s not any&quot; due to a sigmoid activation in the loss function . class BCEWithLogitsLossFlat(BaseLoss): &quot;Same as `nn.BCEWithLogitsLoss`, but flattens input and target.&quot; @use_kwargs_dict(keep=True, weight=None, reduction=&#39;mean&#39;, pos_weight=None) def __init__(self, *args, axis=-1, floatify=True, thresh=0.5, **kwargs): if kwargs.get(&#39;pos_weight&#39;, None) is not None and kwargs.get(&#39;flatten&#39;, None) is True: raise ValueError(&quot;`flatten` must be False when using `pos_weight` to avoid a RuntimeError due to shape mismatch&quot;) if kwargs.get(&#39;pos_weight&#39;, None) is not None: kwargs[&#39;flatten&#39;] = False super().__init__(nn.BCEWithLogitsLoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs) self.thresh = thresh def decodes(self, x): return x&gt;self.thresh def activation(self, x): return torch.sigmoid(x) . learn.loss_func = BCEWithLogitsLossFlat() . We&#39;ll find a good learning rate . learn.lr_find() . SuggestedLRs(lr_min=0.03019951581954956, lr_steep=0.03981071710586548) . And train! (we&#39;ll also used mixed precision too!) . lr = 1e-2 learn = learn.to_fp16() . learn.fit_one_cycle(5, slice(lr)) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.924233 | 0.760086 | 0.632852 | 00:04 | . 1 | 0.705390 | 0.285141 | 0.915201 | 00:03 | . 2 | 0.500431 | 0.178712 | 0.935930 | 00:03 | . 3 | 0.383179 | 0.177389 | 0.935930 | 00:03 | . 4 | 0.314030 | 0.172159 | 0.935930 | 00:03 | . Great! Let&#39;s save and unfreeze to train the rest of our model! . learn.save(&#39;stage-1&#39;) . Path(&#39;models/stage-1.pth&#39;) . learn.unfreeze() learn.lr_find() . SuggestedLRs(lr_min=3.981071640737355e-05, lr_steep=9.12010818865383e-07) . We can now go through and use a learning rate around 1e-5 or so, and then have an ending learning rate five times smaller than our starting . learn.fit_one_cycle(5, slice(1e-5, lr/5)) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.163868 | 0.154094 | 0.944095 | 00:03 | . 1 | 0.148674 | 0.138503 | 0.947864 | 00:03 | . 2 | 0.138569 | 0.137463 | 0.949121 | 00:03 | . 3 | 0.128286 | 0.141320 | 0.945980 | 00:03 | . 4 | 0.121801 | 0.138059 | 0.948807 | 00:03 | . learn.show_results(figsize=(15,15)) .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastai/multilabel/classification/2021/08/09/Multi-Label-Classification.html",
            "relUrl": "/fastai/multilabel/classification/2021/08/09/Multi-Label-Classification.html",
            "date": " • Aug 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Working with fastai2 - Low-Level API",
            "content": "Imports . These are the imports for everything we&#39;ll be using in this notebook . from torch import nn from fastai.vision.all import * from fastai.callback.hook import summary from fastai.callback.schedule import fit_one_cycle, lr_find from fastai.callback.progress import ProgressCallback from fastai.data.core import Datasets, DataLoaders, show_at from fastai.data.external import untar_data, URLs from fastai.data.transforms import Categorize, GrandparentSplitter, parent_label, ToTensor, IntToFloatTensor, Normalize from fastai.layers import Flatten from fastai.learner import Learner from fastai.metrics import accuracy, CrossEntropyLossFlat from fastai.vision.augment import CropPad, RandomCrop, PadMode from fastai.vision.core import PILImageBW from fastai.vision.utils import get_image_files . import matplotlib.pyplot as plt plt.style.use(&#39;dark_background&#39;) . . This article is also a Jupyter Notebook available to be run from the top down. There will be code snippets that you can then run in any environment. . Below are the versions of fastai, fastcore, and wwf currently running at the time of writing this: . fastai : 2.2.5 | fastcore : 1.3.12 | wwf : 0.0.16 | . . grabbing our data . path = untar_data(URLs.MNIST) . Working with the data . items = get_image_files(path) . items[0] . Path(&#39;/home/jmtzt/.fastai/data/mnist_png/testing/9/2934.png&#39;) . im = PILImageBW.create(items[0]); im.show() . &lt;AxesSubplot:&gt; . Split our data with GrandparentSplitter, which will make use of a train and valid folder. . splits = GrandparentSplitter(train_name=&#39;training&#39;, valid_name=&#39;testing&#39;) . splits = splits(items) . splits[0][:5], splits[1][:5] . ([10000, 10001, 10002, 10003, 10004], [0, 1, 2, 3, 4]) . Make a Datasets . | Expects items, transforms for describing our problem, and a splitting method . | . dsrc = Datasets(items, tfms=[[PILImageBW.create], [parent_label, Categorize]], splits = splits) . show_at(dsrc.train, 3) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;9&#39;}&gt; . Next we need to give ourselves some transforms on the data! These will need to: . Ensure our images are all the same size | Make sure our output are the tensor our models are wanting | Give some image augmentation | tfms = [ToTensor(), CropPad(size=34, pad_mode=PadMode.Zeros), RandomCrop(size=28)] . ToTensor: Converts to tensor | CropPad and RandomCrop: Resizing transforms | Applied on the CPU via after_item | . gpu_tfms = [IntToFloatTensor(), Normalize()] . IntToFloatTensor: Converts to a float | Normalize: Normalizes data | . dls = dsrc.dataloaders(bs=128, after_item=tfms, after_batch=gpu_tfms) . dls.show_batch() . xb, yb = dls.one_batch() . xb.shape, yb.shape . (torch.Size([128, 1, 28, 28]), torch.Size([128])) . dls.c . 10 . So our input shape will be a [128 x 1 x 28 x 28] and our output shape will be a [128] tensor that we need to condense into 10 classes . Model definition . This model will have 5 convolutional layers | We&#39;ll use nn.Sequential | 1 -&gt; 32 -&gt; 10 | . def conv(ni, nf): return nn.Conv2d(ni, nf, kernel_size=3, stride=2, padding=1) . Here we can see our ni is equivalent to the depth of the filter, and nf is equivalent to how many filters we will be using. (Fun fact this always has to be divisible by the size of our image). . Batch Normalization . As we send our tensors through our model, it is important to normalize our data throughout the network. Doing so can allow for a much larger improvement in training speed, along with allowing each layer to learn independantly (as each layer is then re-normalized according to it&#39;s outputs) . def bn(nf): return nn.BatchNorm2d(nf) . nf will be the same as the filter output from our previous convolutional layer . Activation functions . They give our models non-linearity and work with the weights we mentioned earlier along with a bias through a process called back-propagation. These allow our models to learn and perform more complex tasks because they can choose to fire or activate one of those neurons mentioned earlier. On a simple sense, let&#39;s look at the ReLU activation function. It operates by turning any negative values to zero, as visualized below: . . def ReLU(): return nn.ReLU(inplace=False) . model = nn.Sequential( conv(1, 8), bn(8), ReLU(), conv(8, 16), bn(16), ReLU(), conv(16,32), bn(32), ReLU(), conv(32, 16), bn(16), ReLU(), conv(16, 10), bn(10), Flatten() ) . learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.summary() . Sequential (Input shape: 128) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 128 x 8 x 14 x 14 Conv2d 80 True BatchNorm2d 16 True ReLU ____________________________________________________________________________ 128 x 16 x 7 x 7 Conv2d 1168 True BatchNorm2d 32 True ReLU ____________________________________________________________________________ 128 x 32 x 4 x 4 Conv2d 4640 True BatchNorm2d 64 True ReLU ____________________________________________________________________________ 128 x 16 x 2 x 2 Conv2d 4624 True BatchNorm2d 32 True ReLU ____________________________________________________________________________ 128 x 10 x 1 x 1 Conv2d 1450 True BatchNorm2d 20 True ____________________________________________________________________________ [] Flatten ____________________________________________________________________________ Total params: 12,126 Total trainable params: 12,126 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f56a8917790&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learn.summary also tells us: . Total parameters | Trainable parameters | Optimizer | Loss function | Applied Callbacks | . learn.lr_find() . SuggestedLRs(lr_min=0.33113112449646, lr_steep=0.7585775852203369) . learn.fit_one_cycle(3, lr_max=1e-1) . epoch train_loss valid_loss accuracy time . 0 | 0.210623 | 0.194198 | 0.939300 | 00:09 | . 1 | 0.139447 | 0.079532 | 0.975500 | 00:09 | . 2 | 0.068283 | 0.037102 | 0.987500 | 00:09 | . Simplifying our model . Try to make it more like ResNet. | ConvLayer contains a Conv2d, BatchNorm2d, and an activation function | . def conv2(ni, nf): return ConvLayer(ni, nf, stride=2) . net = nn.Sequential( conv2(1,8), conv2(8,16), conv2(16,32), conv2(32,16), conv2(16,10), Flatten() ) . learn = Learner(dls, net, loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.fit_one_cycle(3, lr_max=1e-1) . epoch train_loss valid_loss accuracy time . 0 | 0.202230 | 0.219875 | 0.931000 | 00:09 | . 1 | 0.130472 | 0.079829 | 0.972500 | 00:09 | . 2 | 0.076784 | 0.038371 | 0.986900 | 00:09 | . ResNet (kind of) . The ResNet architecture is built with what are known as ResBlocks. Each of these blocks consist of two ConvLayers that we made before, where the number of filters do not change. Let&#39;s generate these layers. . class ResBlock(Module): def __init__(self, nf): self.conv1 = ConvLayer(nf, nf) self.conv2 = ConvLayer(nf, nf) def forward(self, x): return x + self.conv2(self.conv1(x)) . Class notation | __init__ | forward | . net = nn.Sequential( conv2(1,8), ResBlock(8), conv2(8,16), ResBlock(16), conv2(16,32), ResBlock(32), conv2(32,16), ResBlock(16), conv2(16,10), Flatten() ) . Awesome! We&#39;re building a pretty substantial model here. Let&#39;s try to make it even simpler. We know we call a convolutional layer before each ResBlock and they all have the same filters, so let&#39;s make that layer! . def conv_and_res(ni, nf): return nn.Sequential(conv2(ni, nf), ResBlock(nf)) . net = nn.Sequential( conv_and_res(1,8), conv_and_res(8,16), conv_and_res(16,32), conv_and_res(32,16), conv2(16,10), Flatten() ) . And now we have something that resembles a ResNet! Let&#39;s see how it performs . learn = Learner(dls, net, loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . SuggestedLRs(lr_min=0.15848932266235352, lr_steep=0.2089296132326126) . learn.fit_one_cycle(3, lr_max=1e-1) . epoch train_loss valid_loss accuracy time . 0 | 0.154220 | 0.295265 | 0.907000 | 00:10 | . 1 | 0.087304 | 0.072216 | 0.976400 | 00:10 | . 2 | 0.041664 | 0.023510 | 0.992200 | 00:10 | . learn.path = Path(&#39;&#39;) learn.export(fname=&#39;export.pkl&#39;) .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastai/api/2021/08/08/fastai-Low-Level-API.html",
            "relUrl": "/fastai/api/2021/08/08/fastai-Low-Level-API.html",
            "date": " • Aug 8, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Stochastic Gradient Descent (SGD)",
            "content": "Imports . . This article is also a Jupyter Notebook available to be run from the top down. There will be code snippets that you can then run in any environment. . Below are the versions of fastai, fastcore, and wwf currently running at the time of writing this: . fastai : 2.2.5 | fastcore : 1.3.12 | wwf : 0.0.16 | . . import matplotlib.pyplot as plt plt.style.use(&#39;dark_background&#39;) . from fastai.basics import * import torch from torch import nn import numpy as np import matplotlib.pyplot as plt from fastai.torch_core import tensor . Linear Regression . Fit a line on 100 points | . n = 100 . x = torch.ones(n, 2) len(x), x[:5] . (100, tensor([[1., 1.], [1., 1.], [1., 1.], [1., 1.], [1., 1.]])) . randomize in an uniform distribution from -1 to 1 . x[:,0].uniform_(-1., 1) x[:5], x.shape . (tensor([[0.6555, 1.0000], [0.0426, 1.0000], [0.2065, 1.0000], [0.4251, 1.0000], [0.9636, 1.0000]]), torch.Size([100, 2])) . Any linear model is y=mx+b | m, x, and b are matrices | We have x | . m = tensor(3.,2); m, m.shape . (tensor([3., 2.]), torch.Size([2])) . b is a random bias | . b = torch.rand(n); b[:5], b.shape . (tensor([0.4444, 0.2204, 0.3399, 0.5224, 0.1004]), torch.Size([100])) . Now we can make our y . Matrix multiplication is denoted with @ | . y = x@m + b . We&#39;ll know if we got a size wrong if: . m@x + b . RuntimeError Traceback (most recent call last) &lt;ipython-input-11-ac53957f9814&gt; in &lt;module&gt; -&gt; 1 m@x + b RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x2 and 100x2) . Plot our results . plt.scatter(x[:,0], y) . &lt;matplotlib.collections.PathCollection at 0x7f102872cd90&gt; . Our weights from last lesson should minimize the distance between points and our line. . mean squared error: Take distance from pred and y, square, then average | . def mse(y_hat, y): return ((y_hat-y)**2).mean() . When we run our model, we are trying to predict m . For example, say a = (0.5, 0.75). . Make a prediction | Calculate the error | . a = tensor(.5, .75) . Make prediction . y_pred = x@a . Calculate error . mse(y_pred, y) . tensor(5.2860) . plt.scatter(x[:,0],y) plt.scatter(x[:,0],y_pred) . &lt;matplotlib.collections.PathCollection at 0x7f0f4da4a430&gt; . Model doesn&#39;t seen to quite fit. What&#39;s next? Optimization . Walking down Gradient Descent . Goal: Minimize the loss function (mse) | Gradient Descent: Starts with parameters | Moves towards new parameters to minimize the function | Take steps in the negative direction of gradient function | . | . a = nn.Parameter(a); a . Parameter containing: tensor([0.5000, 0.7500], requires_grad=True) . Next let&#39;s create an update function to check if the current a improved. If so, move even closer. . We&#39;ll print out every 10 iterations to see how we are doing . def update(): y_hat = x@a loss = mse(y, y_hat) if i % 10 == 0: print(loss) loss.backward() with torch.no_grad(): a.sub_(lr * a.grad) a.grad.zero_() . torch.no_grad: No back propogation (no updating of our weights) | sub_: Subtracts some value (lr * our gradient) | grad.zero_: Zeros our gradients | . lr = 1e-1 . for i in range(100): update() . tensor(5.2860, grad_fn=&lt;MeanBackward0&gt;) tensor(0.5746, grad_fn=&lt;MeanBackward0&gt;) tensor(0.1852, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0990, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0781, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0730, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0717, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0714, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0713, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0713, grad_fn=&lt;MeanBackward0&gt;) . Now let&#39;s see how this new a compares. . Detach removes all gradients | . plt.scatter(x[:,0],y) plt.scatter(x[:,0], (x@a).detach()) plt.scatter(x[:,0],y_pred) . &lt;matplotlib.collections.PathCollection at 0x7f0f4da29c40&gt; . We fit our line much better here . Animate the process . from matplotlib import animation, rc rc(&#39;animation&#39;, html=&#39;jshtml&#39;) . a = nn.Parameter(tensor(0.5, 0.75)); a . Parameter containing: tensor([0.5000, 0.7500], requires_grad=True) . def animate(i): update() line.set_ydata((x@a).detach()) return line, . fig = plt.figure() plt.scatter(x[:,0], y, c=&#39;orange&#39;) line, = plt.plot(x[:,0], (x@a).detach()) plt.close() . animation.FuncAnimation(fig, animate, np.arange(0,100), interval=20) . &lt;/input&gt; Once Loop Reflect Ideally we split things up into batches of data to fit, and then work with all those batches (else we&#39;d run out of memory! . If this were a classification problem, we would want to use Cross Entropy Loss, where we penalize incorrect confident predictions along with correct unconfident predictions. It&#39;s also called negative loss likelihood .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastai/api/2021/08/08/Stochastic-Gradient-Descent.html",
            "relUrl": "/fastai/api/2021/08/08/Stochastic-Gradient-Descent.html",
            "date": " • Aug 8, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Experiments with State-of-the-Art techniques",
            "content": "Imports . from fastai2.vision.all import * from utils import * path = Path(&#39;./DDSM_NOBARS/MASS/&#39;); path.ls() . (#2) [Path(&#39;DDSM_NOBARS/MASS/benigna&#39;),Path(&#39;DDSM_NOBARS/MASS/maligna&#39;)] . DataBlock/DataLoaders . dblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(seed=42), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) #dblock.summary(path) dls = dblock.dataloaders(path, bs=64) . Baseline run . model = resnet34() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate]).to_fp16() learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 7.298484 | 49.292488 | 0.000000 | 1.000000 | 00:06 | . 1 | 3.581588 | 1.451827 | 0.404858 | 0.595142 | 00:06 | . 2 | 2.321033 | 0.743234 | 0.461538 | 0.538462 | 00:06 | . 3 | 1.713093 | 0.695602 | 0.502024 | 0.497976 | 00:06 | . 4 | 1.362898 | 0.681767 | 0.595142 | 0.404858 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Normalization . Testing if the data is normalized i.e. has a mean of 0 and a std of 1. . x,y = dls.one_batch() x.shape,y.shape . (torch.Size([64, 3, 224, 224]), torch.Size([64])) . x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.5666, 0.5666, 0.5666], device=&#39;cuda:0&#39;), TensorImage([0.1686, 0.1686, 0.1686], device=&#39;cuda:0&#39;)) . The mean and standart deviation are not close to the desired values, therefore we need to normalize them by adding to the DataBlock the Normalize transform that uses the ImageNet mean and std. . def get_dls(bs, size): dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(seed=42), item_tfms=Resize(460), batch_tfms=[*aug_transforms(size=size, min_scale=0.75), Normalize.from_stats(*imagenet_stats)]) return dblock.dataloaders(path, bs=bs) . dls = get_dls(64, 224) x,y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.3459, 0.4831, 0.7032], device=&#39;cuda:0&#39;), TensorImage([0.7476, 0.7643, 0.7609], device=&#39;cuda:0&#39;)) . Checking if it affects our model . model = resnet34() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate]).to_fp16() learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 6.589247 | nan | 0.534413 | 0.465587 | 00:06 | . 1 | 3.267678 | 1.586851 | 0.615385 | 0.384615 | 00:06 | . 2 | 2.144929 | 1.013776 | 0.599190 | 0.400810 | 00:06 | . 3 | 1.608513 | 0.705930 | 0.437247 | 0.562753 | 00:06 | . 4 | 1.298861 | 0.669487 | 0.619433 | 0.380567 | 00:06 | . We can see some improvement in our acccuracy, but not a whole lot, because we are training this model from scratch. If we were to train this model based on a transfer learning approach, we would have to pay even more attention to that, in order to match the statistics used for normalization of the pre-trained model. . Progressive Resizing . This approach is basically gradually using larger and larger images as you train your model . dls = get_dls(128, 128) learn = Learner(dls, resnet34(), loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate]).to_fp16() learn.fit_one_cycle(4, 3e-3) . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . epoch train_loss valid_loss accuracy error_rate time . 0 | 7.789251 | 7.220915 | 0.000000 | 1.000000 | 00:05 | . 1 | 6.119124 | nan | 0.570850 | 0.429150 | 00:05 | . 2 | 4.074199 | nan | 0.562753 | 0.437247 | 00:05 | . 3 | 3.038997 | 5.341589 | 0.518219 | 0.481781 | 00:06 | . We increase the image resolution and decrease the batch size and proceed to fine-tune the model: . learn.dls = get_dls(64, 224) learn.fine_tune(5, 1e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 0.699829 | 0.706627 | 0.510121 | 0.489879 | 00:06 | . epoch train_loss valid_loss accuracy error_rate time . 0 | 0.677542 | 0.713452 | 0.599190 | 0.400810 | 00:06 | . 1 | 0.681381 | 0.686060 | 0.635628 | 0.364372 | 00:06 | . 2 | 0.682831 | 0.699931 | 0.510121 | 0.489879 | 00:06 | . 3 | 0.685403 | 0.668482 | 0.619433 | 0.380567 | 00:06 | . 4 | 0.682514 | 0.671242 | 0.663968 | 0.336032 | 00:06 | . As we can see, the accuracy improved when using this technique. . Test Time Augmentation (TTA) . Up until now we have been using random cropping as data augmentation, which may lead to some problems such as some critical features being cropped out of the image. One technique that might help mitigate this problem is select a number of areas to crop from the original rectangular image and then pass each of them through the model and take the average of the predictions, that is just applying a form of augmentation in the validation dataset as well. . preds,targs = learn.tta() accuracy(preds, targs).item() . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . 0.659919023513794 . Mixup Technique . mixup: Beyond Empirical Risk Minimization: https://arxiv.org/abs/1710.09412 . Mixup works as follows, for each image: . Select another image from your dataset at random. | Pick a weight at random. | Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable. | Take a weighted average (with the same weight) of this image&#39;s labels with your image&#39;s labels; this will be your dependent variable. | ben = PILImage.create(get_image_files_sorted(path/&#39;benigna&#39;)[0]) mal = PILImage.create(get_image_files_sorted(path/&#39;maligna&#39;)[0]) ben = ben.resize((256,256)) mal = mal.resize((256,256)) tben = tensor(ben).float() / 255. tmal = tensor(mal).float() / 255. _,axs = plt.subplots(1, 3, figsize=(12,4)) show_image(tben, ax=axs[0]); show_image(tmal, ax=axs[1]); show_image((0.3*tben + 0.7*tmal), ax=axs[2]); . The third image is 30% the first one and 70% the second one, so the model must predict 30% benign and 70% malign. So the one-hot-encoded representations of the predicitons are (in this dataset which we have 2 classes): . [1,0] and [0, 1] . But we are aiming for this type of prediction: . [0.3, 0.7] . model = resnet34() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate], cbs=MixUp) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 4.664900 | 208.644165 | 0.591093 | 0.408907 | 00:07 | . 1 | 2.637848 | 194.706757 | 0.591093 | 0.408907 | 00:06 | . 2 | 1.903277 | 1.384996 | 0.526316 | 0.473684 | 00:06 | . 3 | 1.535061 | 0.859590 | 0.534413 | 0.465587 | 00:06 | . 4 | 1.315490 | 0.679021 | 0.615385 | 0.384615 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Training in this way, without explicitly telling the model that the labels must be biggen than 0 but smaller than 1, makes our activations more extreme as we train for more epochs. That is the reason why we will use label smoothing to deal with this. . Label Smoothing . Rethinking the Inception Architecture for Computer Vision: https://arxiv.org/abs/1512.00567 . Instead of using regular one-hot-encoded vectors for the targets, we should use targets in the following format: . [0.1, 0.9] . This helps we do not encourage the model to predict something overconfidently. . model = resnet34() learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=[accuracy, error_rate], cbs=MixUp) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 4.738817 | 4001.799072 | 0.000000 | 1.000000 | 00:07 | . 1 | 3.226263 | 1226.570801 | 0.591093 | 0.408907 | 00:07 | . 2 | 2.642515 | 60.685635 | 0.518219 | 0.481781 | 00:06 | . 3 | 2.353480 | 4.580351 | 0.554656 | 0.445344 | 00:06 | . 4 | 2.170954 | 1.858582 | 0.530364 | 0.469636 | 00:07 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Normally we see improvements both from the MixUp and the Label Smoothing technique when we train the model for more epochs. .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastpages/jupyter/fastai/ddsm/mass/2020/05/30/DDSM-Experiments.html",
            "relUrl": "/fastpages/jupyter/fastai/ddsm/mass/2020/05/30/DDSM-Experiments.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Cross-Validation with fastai v2",
            "content": "from fastai2.vision.all import * path = Path(&#39;./DDSM_NOBARS/&#39;); path.ls() . DDSM Mass Dataset and splits . batch_tfms = [IntToFloatTensor(), *aug_transforms(size=224, max_warp=0, min_scale=0.75), Normalize.from_stats(*imagenet_stats)] item_tfms = [ToTensor(), Resize(460)] bs=64 . train_imgs = get_image_files(path/&#39;train&#39;) tst_imgs = get_image_files(path/&#39;test&#39;) . random.shuffle(train_imgs) . len(train_imgs) . 1239 . Here we do an 80/20 split . start_val = len(train_imgs) - int(len(train_imgs)*.2) # last 20% validation . idxs = list(range(start_val, len(train_imgs))) . splits = IndexSplitter(idxs) . split = splits(train_imgs) . len(train_imgs) . 1239 . split_list = [split[0], split[1]] . split_list.append(L(range(len(train_imgs), len(train_imgs)+len(tst_imgs)))) . We have 992 training images, 247 validation images and 142 test images . split_list . [(#992) [0,1,2,3,4,5,6,7,8,9...], (#247) [992,993,994,995,996,997,998,999,1000,1001...], (#142) [1239,1240,1241,1242,1243,1244,1245,1246,1247,1248...]] . dsrc = Datasets(train_imgs+tst_imgs, tfms=[[PILImage.create], [parent_label, Categorize]], splits = split_list) . show_at(dsrc.train, 3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8fa00900d0&gt; . dsrc.n_subsets . 3 . dls = dsrc.dataloaders(bs=bs, after_item=item_tfms, after_batch=batch_tfms) . dls.show_batch() . Baseline . learn = cnn_learner(dls, resnet34, pretrained=True, metrics=accuracy).to_fp16() learn.fit_one_cycle(5) . epoch train_loss valid_loss accuracy time . 0 | 1.255100 | 1.078973 | 0.546559 | 00:06 | . 1 | 1.164489 | 1.024763 | 0.562753 | 00:05 | . 2 | 1.068163 | 0.723718 | 0.684211 | 00:06 | . 3 | 1.019540 | 0.661954 | 0.700405 | 00:06 | . 4 | 0.953290 | 0.638257 | 0.712551 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . The baseline model got and accuracy of ~61.27% on the test set. . learn.validate(ds_idx=2)[1] . 0.6126760840415955 . Cross Validation . from sklearn.model_selection import StratifiedKFold . train_labels = L() for i in range(len(dsrc.train)): train_labels.append(dsrc.train[i][1]) for i in range(len(dsrc.valid)): train_labels.append(dsrc.valid[i][1]) train_labels . (#1239) [TensorCategory(1),TensorCategory(1),TensorCategory(1),TensorCategory(0),TensorCategory(0),TensorCategory(0),TensorCategory(1),TensorCategory(0),TensorCategory(1),TensorCategory(1)...] . random.shuffle(train_imgs) . Our training loop will include the same process we&#39;ve done above on the DDSM Mass Dataset and splits section. That is, we will be splitting the training dataset 10 times and training a model on each of these datasets, after the training process is done we take the average of all the predictions for the test set on each split. . val_pct = [] tst_preds = [] skf = StratifiedKFold(n_splits=10, shuffle=True) for _, val_idx in skf.split(np.array(train_imgs), train_labels): splits = IndexSplitter(val_idx) split = splits(train_imgs) split_list = [split[0], split[1]] split_list.append(L(range(len(train_imgs), len(train_imgs)+len(tst_imgs)))) dsrc = Datasets(train_imgs+tst_imgs, tfms=[[PILImage.create], [parent_label, Categorize]], splits=split_list) dls = dsrc.dataloaders(bs=bs, after_item=item_tfms, after_batch=batch_tfms) learn = cnn_learner(dls, resnet34, pretrained=True, metrics=accuracy).to_fp16() learn.fit_one_cycle(5) val_pct.append(learn.validate()[1]) a,b = learn.get_preds(ds_idx=2) tst_preds.append(a) . . epoch train_loss valid_loss accuracy time . 0 | 1.136114 | 0.953437 | 0.491935 | 00:06 | . 1 | 1.066459 | 1.036575 | 0.548387 | 00:05 | . 2 | 1.011496 | 0.784486 | 0.669355 | 00:05 | . 3 | 0.968455 | 0.714560 | 0.669355 | 00:05 | . 4 | 0.928864 | 0.756449 | 0.653226 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . epoch train_loss valid_loss accuracy time . 0 | 1.343768 | 0.991764 | 0.572581 | 00:05 | . 1 | 1.203773 | 0.972596 | 0.629032 | 00:05 | . 2 | 1.097104 | 0.879680 | 0.669355 | 00:06 | . 3 | 1.015946 | 0.630741 | 0.725806 | 00:06 | . 4 | 0.939799 | 0.595138 | 0.733871 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.401825 | 0.975166 | 0.564516 | 00:05 | . 1 | 1.211842 | 1.150666 | 0.620968 | 00:05 | . 2 | 1.106027 | 0.824127 | 0.693548 | 00:05 | . 3 | 1.005552 | 0.643290 | 0.717742 | 00:05 | . 4 | 0.949984 | 0.611353 | 0.685484 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.285932 | 1.116190 | 0.516129 | 00:05 | . 1 | 1.176994 | 0.952720 | 0.629032 | 00:05 | . 2 | 1.059058 | 0.891377 | 0.637097 | 00:06 | . 3 | 0.987869 | 0.730751 | 0.701613 | 00:06 | . 4 | 0.942768 | 0.642452 | 0.701613 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.271043 | 0.824401 | 0.540323 | 00:05 | . 1 | 1.188266 | 1.026128 | 0.564516 | 00:05 | . 2 | 1.083751 | 0.844839 | 0.596774 | 00:05 | . 3 | 1.015182 | 0.688445 | 0.669355 | 00:06 | . 4 | 0.949915 | 0.696868 | 0.677419 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.252791 | 0.952793 | 0.475806 | 00:05 | . 1 | 1.091202 | 0.878849 | 0.596774 | 00:05 | . 2 | 1.030211 | 0.801414 | 0.661290 | 00:06 | . 3 | 0.971240 | 0.716754 | 0.693548 | 00:05 | . 4 | 0.923688 | 0.656634 | 0.717742 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.171420 | 0.854203 | 0.548387 | 00:05 | . 1 | 1.043760 | 0.852307 | 0.637097 | 00:06 | . 2 | 0.946355 | 0.857245 | 0.604839 | 00:05 | . 3 | 0.898549 | 0.815291 | 0.645161 | 00:05 | . 4 | 0.841311 | 0.815915 | 0.677419 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.348083 | 0.972769 | 0.564516 | 00:05 | . 1 | 1.179842 | 0.945408 | 0.620968 | 00:05 | . 2 | 1.112100 | 0.715282 | 0.653226 | 00:05 | . 3 | 1.044266 | 0.638139 | 0.717742 | 00:05 | . 4 | 0.984065 | 0.619281 | 0.717742 | 00:06 | . epoch train_loss valid_loss accuracy time . 0 | 1.230497 | 0.981037 | 0.500000 | 00:06 | . 1 | 1.114652 | 0.823075 | 0.653226 | 00:06 | . 2 | 1.009943 | 0.715533 | 0.653226 | 00:06 | . 3 | 0.941501 | 0.730544 | 0.629032 | 00:06 | . 4 | 0.899372 | 0.703373 | 0.629032 | 00:06 | . epoch train_loss valid_loss accuracy time . 0 | 1.403852 | 0.920186 | 0.552846 | 00:06 | . 1 | 1.189877 | 0.931247 | 0.658537 | 00:05 | . 2 | 1.064095 | 0.824453 | 0.658537 | 00:05 | . 3 | 1.018285 | 0.733249 | 0.642276 | 00:05 | . 4 | 0.971781 | 0.751345 | 0.650406 | 00:05 | . Now how do we combine all our predictions? We sum them all together then divide by our total . tst_preds_copy = tst_preds.copy() . accuracy(tst_preds_copy[0], b) . tensor(0.6268) . Now let&#39;s add them all together and get an average prediction . for i in tst_preds_copy: print(accuracy(i, b)) . tensor(0.6479) tensor(0.5986) tensor(0.6268) tensor(0.6127) tensor(0.6408) tensor(0.6127) tensor(0.6549) tensor(0.6338) tensor(0.6831) tensor(0.6408) . hat = tst_preds[0] for pred in tst_preds[1:]: hat += pred . hat[:5] . tensor([[3.7651, 6.2349], [9.1702, 0.8298], [8.2053, 1.7947], [9.8177, 0.1823], [4.1707, 5.8293]]) . len(hat) . 142 . hat /= len(tst_preds) . This accuracy is the average of all the predictions of the models trained on the splits. ~~65.49% . accuracy(hat, b) . tensor(0.6549) . This is the process we&#39;ll be using to conduct our experiments on several CNN models. .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastpages/jupyter/fastai/ddsm/mass/2020/04/06/CrossValidation.html",
            "relUrl": "/fastpages/jupyter/fastai/ddsm/mass/2020/04/06/CrossValidation.html",
            "date": " • Apr 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jmtzt.github.io/jmtzt_notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jmtzt.github.io/jmtzt_notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}