{
  
    
        "post0": {
            "title": "Working with fastai2 - Low-Level API",
            "content": "Imports . These are the imports for everything we&#39;ll be using in this notebook . from torch import nn from fastai.vision.all import * from fastai.callback.hook import summary from fastai.callback.schedule import fit_one_cycle, lr_find from fastai.callback.progress import ProgressCallback from fastai.data.core import Datasets, DataLoaders, show_at from fastai.data.external import untar_data, URLs from fastai.data.transforms import Categorize, GrandparentSplitter, parent_label, ToTensor, IntToFloatTensor, Normalize from fastai.layers import Flatten from fastai.learner import Learner from fastai.metrics import accuracy, CrossEntropyLossFlat from fastai.vision.augment import CropPad, RandomCrop, PadMode from fastai.vision.core import PILImageBW from fastai.vision.utils import get_image_files . import matplotlib.pyplot as plt plt.style.use(&#39;dark_background&#39;) . . This article is also a Jupyter Notebook available to be run from the top down. There will be code snippets that you can then run in any environment. . Below are the versions of fastai, fastcore, and wwf currently running at the time of writing this: . fastai : 2.2.5 | fastcore : 1.3.12 | wwf : 0.0.16 | . . grabbing our data . path = untar_data(URLs.MNIST) . Working with the data . items = get_image_files(path) . items[0] . Path(&#39;/home/jmtzt/.fastai/data/mnist_png/testing/9/2934.png&#39;) . im = PILImageBW.create(items[0]); im.show() . &lt;AxesSubplot:&gt; . Split our data with GrandparentSplitter, which will make use of a train and valid folder. . splits = GrandparentSplitter(train_name=&#39;training&#39;, valid_name=&#39;testing&#39;) . splits = splits(items) . splits[0][:5], splits[1][:5] . ([10000, 10001, 10002, 10003, 10004], [0, 1, 2, 3, 4]) . Make a Datasets . | Expects items, transforms for describing our problem, and a splitting method . | . dsrc = Datasets(items, tfms=[[PILImageBW.create], [parent_label, Categorize]], splits = splits) . show_at(dsrc.train, 3) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;9&#39;}&gt; . Next we need to give ourselves some transforms on the data! These will need to: . Ensure our images are all the same size | Make sure our output are the tensor our models are wanting | Give some image augmentation | tfms = [ToTensor(), CropPad(size=34, pad_mode=PadMode.Zeros), RandomCrop(size=28)] . ToTensor: Converts to tensor | CropPad and RandomCrop: Resizing transforms | Applied on the CPU via after_item | . gpu_tfms = [IntToFloatTensor(), Normalize()] . IntToFloatTensor: Converts to a float | Normalize: Normalizes data | . dls = dsrc.dataloaders(bs=128, after_item=tfms, after_batch=gpu_tfms) . dls.show_batch() . xb, yb = dls.one_batch() . xb.shape, yb.shape . (torch.Size([128, 1, 28, 28]), torch.Size([128])) . dls.c . 10 . So our input shape will be a [128 x 1 x 28 x 28] and our output shape will be a [128] tensor that we need to condense into 10 classes . Model definition . This model will have 5 convolutional layers | We&#39;ll use nn.Sequential | 1 -&gt; 32 -&gt; 10 | . def conv(ni, nf): return nn.Conv2d(ni, nf, kernel_size=3, stride=2, padding=1) . Here we can see our ni is equivalent to the depth of the filter, and nf is equivalent to how many filters we will be using. (Fun fact this always has to be divisible by the size of our image). . Batch Normalization . As we send our tensors through our model, it is important to normalize our data throughout the network. Doing so can allow for a much larger improvement in training speed, along with allowing each layer to learn independantly (as each layer is then re-normalized according to it&#39;s outputs) . def bn(nf): return nn.BatchNorm2d(nf) . nf will be the same as the filter output from our previous convolutional layer . Activation functions . They give our models non-linearity and work with the weights we mentioned earlier along with a bias through a process called back-propagation. These allow our models to learn and perform more complex tasks because they can choose to fire or activate one of those neurons mentioned earlier. On a simple sense, let&#39;s look at the ReLU activation function. It operates by turning any negative values to zero, as visualized below: . . def ReLU(): return nn.ReLU(inplace=False) . model = nn.Sequential( conv(1, 8), bn(8), ReLU(), conv(8, 16), bn(16), ReLU(), conv(16,32), bn(32), ReLU(), conv(32, 16), bn(16), ReLU(), conv(16, 10), bn(10), Flatten() ) . learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.summary() . Sequential (Input shape: 128) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 128 x 8 x 14 x 14 Conv2d 80 True BatchNorm2d 16 True ReLU ____________________________________________________________________________ 128 x 16 x 7 x 7 Conv2d 1168 True BatchNorm2d 32 True ReLU ____________________________________________________________________________ 128 x 32 x 4 x 4 Conv2d 4640 True BatchNorm2d 64 True ReLU ____________________________________________________________________________ 128 x 16 x 2 x 2 Conv2d 4624 True BatchNorm2d 32 True ReLU ____________________________________________________________________________ 128 x 10 x 1 x 1 Conv2d 1450 True BatchNorm2d 20 True ____________________________________________________________________________ [] Flatten ____________________________________________________________________________ Total params: 12,126 Total trainable params: 12,126 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f56a8917790&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learn.summary also tells us: . Total parameters | Trainable parameters | Optimizer | Loss function | Applied Callbacks | . learn.lr_find() . SuggestedLRs(lr_min=0.33113112449646, lr_steep=0.7585775852203369) . learn.fit_one_cycle(3, lr_max=1e-1) . epoch train_loss valid_loss accuracy time . 0 | 0.210623 | 0.194198 | 0.939300 | 00:09 | . 1 | 0.139447 | 0.079532 | 0.975500 | 00:09 | . 2 | 0.068283 | 0.037102 | 0.987500 | 00:09 | . Simplifying our model . Try to make it more like ResNet. | ConvLayer contains a Conv2d, BatchNorm2d, and an activation function | . def conv2(ni, nf): return ConvLayer(ni, nf, stride=2) . net = nn.Sequential( conv2(1,8), conv2(8,16), conv2(16,32), conv2(32,16), conv2(16,10), Flatten() ) . learn = Learner(dls, net, loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.fit_one_cycle(3, lr_max=1e-1) . epoch train_loss valid_loss accuracy time . 0 | 0.202230 | 0.219875 | 0.931000 | 00:09 | . 1 | 0.130472 | 0.079829 | 0.972500 | 00:09 | . 2 | 0.076784 | 0.038371 | 0.986900 | 00:09 | . ResNet (kind of) . The ResNet architecture is built with what are known as ResBlocks. Each of these blocks consist of two ConvLayers that we made before, where the number of filters do not change. Let&#39;s generate these layers. . class ResBlock(Module): def __init__(self, nf): self.conv1 = ConvLayer(nf, nf) self.conv2 = ConvLayer(nf, nf) def forward(self, x): return x + self.conv2(self.conv1(x)) . Class notation | __init__ | forward | . net = nn.Sequential( conv2(1,8), ResBlock(8), conv2(8,16), ResBlock(16), conv2(16,32), ResBlock(32), conv2(32,16), ResBlock(16), conv2(16,10), Flatten() ) . Awesome! We&#39;re building a pretty substantial model here. Let&#39;s try to make it even simpler. We know we call a convolutional layer before each ResBlock and they all have the same filters, so let&#39;s make that layer! . def conv_and_res(ni, nf): return nn.Sequential(conv2(ni, nf), ResBlock(nf)) . net = nn.Sequential( conv_and_res(1,8), conv_and_res(8,16), conv_and_res(16,32), conv_and_res(32,16), conv2(16,10), Flatten() ) . And now we have something that resembles a ResNet! Let&#39;s see how it performs . learn = Learner(dls, net, loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . SuggestedLRs(lr_min=0.15848932266235352, lr_steep=0.2089296132326126) . learn.fit_one_cycle(3, lr_max=1e-1) . epoch train_loss valid_loss accuracy time . 0 | 0.154220 | 0.295265 | 0.907000 | 00:10 | . 1 | 0.087304 | 0.072216 | 0.976400 | 00:10 | . 2 | 0.041664 | 0.023510 | 0.992200 | 00:10 | . learn.path = Path(&#39;&#39;) learn.export(fname=&#39;export.pkl&#39;) .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastai/api/2021/08/08/fastai-Low-Level-API.html",
            "relUrl": "/fastai/api/2021/08/08/fastai-Low-Level-API.html",
            "date": " • Aug 8, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Stochastic Gradient Descent (SGD)",
            "content": "Imports . . This article is also a Jupyter Notebook available to be run from the top down. There will be code snippets that you can then run in any environment. . Below are the versions of fastai, fastcore, and wwf currently running at the time of writing this: . fastai : 2.2.5 | fastcore : 1.3.12 | wwf : 0.0.16 | . . import matplotlib.pyplot as plt plt.style.use(&#39;dark_background&#39;) . from fastai.basics import * import torch from torch import nn import numpy as np import matplotlib.pyplot as plt from fastai.torch_core import tensor . Linear Regression . Fit a line on 100 points | . n = 100 . x = torch.ones(n, 2) len(x), x[:5] . (100, tensor([[1., 1.], [1., 1.], [1., 1.], [1., 1.], [1., 1.]])) . randomize in an uniform distribution from -1 to 1 . x[:,0].uniform_(-1., 1) x[:5], x.shape . (tensor([[0.6555, 1.0000], [0.0426, 1.0000], [0.2065, 1.0000], [0.4251, 1.0000], [0.9636, 1.0000]]), torch.Size([100, 2])) . Any linear model is y=mx+b | m, x, and b are matrices | We have x | . m = tensor(3.,2); m, m.shape . (tensor([3., 2.]), torch.Size([2])) . b is a random bias | . b = torch.rand(n); b[:5], b.shape . (tensor([0.4444, 0.2204, 0.3399, 0.5224, 0.1004]), torch.Size([100])) . Now we can make our y . Matrix multiplication is denoted with @ | . y = x@m + b . We&#39;ll know if we got a size wrong if: . m@x + b . RuntimeError Traceback (most recent call last) &lt;ipython-input-11-ac53957f9814&gt; in &lt;module&gt; -&gt; 1 m@x + b RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x2 and 100x2) . Plot our results . plt.scatter(x[:,0], y) . &lt;matplotlib.collections.PathCollection at 0x7f102872cd90&gt; . Our weights from last lesson should minimize the distance between points and our line. . mean squared error: Take distance from pred and y, square, then average | . def mse(y_hat, y): return ((y_hat-y)**2).mean() . When we run our model, we are trying to predict m . For example, say a = (0.5, 0.75). . Make a prediction | Calculate the error | . a = tensor(.5, .75) . Make prediction . y_pred = x@a . Calculate error . mse(y_pred, y) . tensor(5.2860) . plt.scatter(x[:,0],y) plt.scatter(x[:,0],y_pred) . &lt;matplotlib.collections.PathCollection at 0x7f0f4da4a430&gt; . Model doesn&#39;t seen to quite fit. What&#39;s next? Optimization . Walking down Gradient Descent . Goal: Minimize the loss function (mse) | Gradient Descent: Starts with parameters | Moves towards new parameters to minimize the function | Take steps in the negative direction of gradient function | . | . a = nn.Parameter(a); a . Parameter containing: tensor([0.5000, 0.7500], requires_grad=True) . Next let&#39;s create an update function to check if the current a improved. If so, move even closer. . We&#39;ll print out every 10 iterations to see how we are doing . def update(): y_hat = x@a loss = mse(y, y_hat) if i % 10 == 0: print(loss) loss.backward() with torch.no_grad(): a.sub_(lr * a.grad) a.grad.zero_() . torch.no_grad: No back propogation (no updating of our weights) | sub_: Subtracts some value (lr * our gradient) | grad.zero_: Zeros our gradients | . lr = 1e-1 . for i in range(100): update() . tensor(5.2860, grad_fn=&lt;MeanBackward0&gt;) tensor(0.5746, grad_fn=&lt;MeanBackward0&gt;) tensor(0.1852, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0990, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0781, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0730, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0717, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0714, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0713, grad_fn=&lt;MeanBackward0&gt;) tensor(0.0713, grad_fn=&lt;MeanBackward0&gt;) . Now let&#39;s see how this new a compares. . Detach removes all gradients | . plt.scatter(x[:,0],y) plt.scatter(x[:,0], (x@a).detach()) plt.scatter(x[:,0],y_pred) . &lt;matplotlib.collections.PathCollection at 0x7f0f4da29c40&gt; . We fit our line much better here . Animate the process . from matplotlib import animation, rc rc(&#39;animation&#39;, html=&#39;jshtml&#39;) . a = nn.Parameter(tensor(0.5, 0.75)); a . Parameter containing: tensor([0.5000, 0.7500], requires_grad=True) . def animate(i): update() line.set_ydata((x@a).detach()) return line, . fig = plt.figure() plt.scatter(x[:,0], y, c=&#39;orange&#39;) line, = plt.plot(x[:,0], (x@a).detach()) plt.close() . animation.FuncAnimation(fig, animate, np.arange(0,100), interval=20) . &lt;/input&gt; Once Loop Reflect Ideally we split things up into batches of data to fit, and then work with all those batches (else we&#39;d run out of memory! . If this were a classification problem, we would want to use Cross Entropy Loss, where we penalize incorrect confident predictions along with correct unconfident predictions. It&#39;s also called negative loss likelihood .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastai/api/2021/08/08/Stochastic-Gradient-Descent.html",
            "relUrl": "/fastai/api/2021/08/08/Stochastic-Gradient-Descent.html",
            "date": " • Aug 8, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Experiments with State-of-the-Art techniques",
            "content": "Imports . from fastai2.vision.all import * from utils import * path = Path(&#39;./DDSM_NOBARS/MASS/&#39;); path.ls() . (#2) [Path(&#39;DDSM_NOBARS/MASS/benigna&#39;),Path(&#39;DDSM_NOBARS/MASS/maligna&#39;)] . DataBlock/DataLoaders . dblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(seed=42), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) #dblock.summary(path) dls = dblock.dataloaders(path, bs=64) . Baseline run . model = resnet34() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate]).to_fp16() learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 7.298484 | 49.292488 | 0.000000 | 1.000000 | 00:06 | . 1 | 3.581588 | 1.451827 | 0.404858 | 0.595142 | 00:06 | . 2 | 2.321033 | 0.743234 | 0.461538 | 0.538462 | 00:06 | . 3 | 1.713093 | 0.695602 | 0.502024 | 0.497976 | 00:06 | . 4 | 1.362898 | 0.681767 | 0.595142 | 0.404858 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Normalization . Testing if the data is normalized i.e. has a mean of 0 and a std of 1. . x,y = dls.one_batch() x.shape,y.shape . (torch.Size([64, 3, 224, 224]), torch.Size([64])) . x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.5666, 0.5666, 0.5666], device=&#39;cuda:0&#39;), TensorImage([0.1686, 0.1686, 0.1686], device=&#39;cuda:0&#39;)) . The mean and standart deviation are not close to the desired values, therefore we need to normalize them by adding to the DataBlock the Normalize transform that uses the ImageNet mean and std. . def get_dls(bs, size): dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(seed=42), item_tfms=Resize(460), batch_tfms=[*aug_transforms(size=size, min_scale=0.75), Normalize.from_stats(*imagenet_stats)]) return dblock.dataloaders(path, bs=bs) . dls = get_dls(64, 224) x,y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.3459, 0.4831, 0.7032], device=&#39;cuda:0&#39;), TensorImage([0.7476, 0.7643, 0.7609], device=&#39;cuda:0&#39;)) . Checking if it affects our model . model = resnet34() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate]).to_fp16() learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 6.589247 | nan | 0.534413 | 0.465587 | 00:06 | . 1 | 3.267678 | 1.586851 | 0.615385 | 0.384615 | 00:06 | . 2 | 2.144929 | 1.013776 | 0.599190 | 0.400810 | 00:06 | . 3 | 1.608513 | 0.705930 | 0.437247 | 0.562753 | 00:06 | . 4 | 1.298861 | 0.669487 | 0.619433 | 0.380567 | 00:06 | . We can see some improvement in our acccuracy, but not a whole lot, because we are training this model from scratch. If we were to train this model based on a transfer learning approach, we would have to pay even more attention to that, in order to match the statistics used for normalization of the pre-trained model. . Progressive Resizing . This approach is basically gradually using larger and larger images as you train your model . dls = get_dls(128, 128) learn = Learner(dls, resnet34(), loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate]).to_fp16() learn.fit_one_cycle(4, 3e-3) . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . epoch train_loss valid_loss accuracy error_rate time . 0 | 7.789251 | 7.220915 | 0.000000 | 1.000000 | 00:05 | . 1 | 6.119124 | nan | 0.570850 | 0.429150 | 00:05 | . 2 | 4.074199 | nan | 0.562753 | 0.437247 | 00:05 | . 3 | 3.038997 | 5.341589 | 0.518219 | 0.481781 | 00:06 | . We increase the image resolution and decrease the batch size and proceed to fine-tune the model: . learn.dls = get_dls(64, 224) learn.fine_tune(5, 1e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 0.699829 | 0.706627 | 0.510121 | 0.489879 | 00:06 | . epoch train_loss valid_loss accuracy error_rate time . 0 | 0.677542 | 0.713452 | 0.599190 | 0.400810 | 00:06 | . 1 | 0.681381 | 0.686060 | 0.635628 | 0.364372 | 00:06 | . 2 | 0.682831 | 0.699931 | 0.510121 | 0.489879 | 00:06 | . 3 | 0.685403 | 0.668482 | 0.619433 | 0.380567 | 00:06 | . 4 | 0.682514 | 0.671242 | 0.663968 | 0.336032 | 00:06 | . As we can see, the accuracy improved when using this technique. . Test Time Augmentation (TTA) . Up until now we have been using random cropping as data augmentation, which may lead to some problems such as some critical features being cropped out of the image. One technique that might help mitigate this problem is select a number of areas to crop from the original rectangular image and then pass each of them through the model and take the average of the predictions, that is just applying a form of augmentation in the validation dataset as well. . preds,targs = learn.tta() accuracy(preds, targs).item() . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . 0.659919023513794 . Mixup Technique . mixup: Beyond Empirical Risk Minimization: https://arxiv.org/abs/1710.09412 . Mixup works as follows, for each image: . Select another image from your dataset at random. | Pick a weight at random. | Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable. | Take a weighted average (with the same weight) of this image&#39;s labels with your image&#39;s labels; this will be your dependent variable. | ben = PILImage.create(get_image_files_sorted(path/&#39;benigna&#39;)[0]) mal = PILImage.create(get_image_files_sorted(path/&#39;maligna&#39;)[0]) ben = ben.resize((256,256)) mal = mal.resize((256,256)) tben = tensor(ben).float() / 255. tmal = tensor(mal).float() / 255. _,axs = plt.subplots(1, 3, figsize=(12,4)) show_image(tben, ax=axs[0]); show_image(tmal, ax=axs[1]); show_image((0.3*tben + 0.7*tmal), ax=axs[2]); . The third image is 30% the first one and 70% the second one, so the model must predict 30% benign and 70% malign. So the one-hot-encoded representations of the predicitons are (in this dataset which we have 2 classes): . [1,0] and [0, 1] . But we are aiming for this type of prediction: . [0.3, 0.7] . model = resnet34() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate], cbs=MixUp) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 4.664900 | 208.644165 | 0.591093 | 0.408907 | 00:07 | . 1 | 2.637848 | 194.706757 | 0.591093 | 0.408907 | 00:06 | . 2 | 1.903277 | 1.384996 | 0.526316 | 0.473684 | 00:06 | . 3 | 1.535061 | 0.859590 | 0.534413 | 0.465587 | 00:06 | . 4 | 1.315490 | 0.679021 | 0.615385 | 0.384615 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Training in this way, without explicitly telling the model that the labels must be biggen than 0 but smaller than 1, makes our activations more extreme as we train for more epochs. That is the reason why we will use label smoothing to deal with this. . Label Smoothing . Rethinking the Inception Architecture for Computer Vision: https://arxiv.org/abs/1512.00567 . Instead of using regular one-hot-encoded vectors for the targets, we should use targets in the following format: . [0.1, 0.9] . This helps we do not encourage the model to predict something overconfidently. . model = resnet34() learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=[accuracy, error_rate], cbs=MixUp) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 4.738817 | 4001.799072 | 0.000000 | 1.000000 | 00:07 | . 1 | 3.226263 | 1226.570801 | 0.591093 | 0.408907 | 00:07 | . 2 | 2.642515 | 60.685635 | 0.518219 | 0.481781 | 00:06 | . 3 | 2.353480 | 4.580351 | 0.554656 | 0.445344 | 00:06 | . 4 | 2.170954 | 1.858582 | 0.530364 | 0.469636 | 00:07 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Normally we see improvements both from the MixUp and the Label Smoothing technique when we train the model for more epochs. .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastpages/jupyter/fastai/ddsm/mass/2020/05/30/DDSM-Experiments.html",
            "relUrl": "/fastpages/jupyter/fastai/ddsm/mass/2020/05/30/DDSM-Experiments.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Cross-Validation with fastai v2",
            "content": "from fastai2.vision.all import * path = Path(&#39;./DDSM_NOBARS/&#39;); path.ls() . DDSM Mass Dataset and splits . batch_tfms = [IntToFloatTensor(), *aug_transforms(size=224, max_warp=0, min_scale=0.75), Normalize.from_stats(*imagenet_stats)] item_tfms = [ToTensor(), Resize(460)] bs=64 . train_imgs = get_image_files(path/&#39;train&#39;) tst_imgs = get_image_files(path/&#39;test&#39;) . random.shuffle(train_imgs) . len(train_imgs) . 1239 . Here we do an 80/20 split . start_val = len(train_imgs) - int(len(train_imgs)*.2) # last 20% validation . idxs = list(range(start_val, len(train_imgs))) . splits = IndexSplitter(idxs) . split = splits(train_imgs) . len(train_imgs) . 1239 . split_list = [split[0], split[1]] . split_list.append(L(range(len(train_imgs), len(train_imgs)+len(tst_imgs)))) . We have 992 training images, 247 validation images and 142 test images . split_list . [(#992) [0,1,2,3,4,5,6,7,8,9...], (#247) [992,993,994,995,996,997,998,999,1000,1001...], (#142) [1239,1240,1241,1242,1243,1244,1245,1246,1247,1248...]] . dsrc = Datasets(train_imgs+tst_imgs, tfms=[[PILImage.create], [parent_label, Categorize]], splits = split_list) . show_at(dsrc.train, 3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8fa00900d0&gt; . dsrc.n_subsets . 3 . dls = dsrc.dataloaders(bs=bs, after_item=item_tfms, after_batch=batch_tfms) . dls.show_batch() . Baseline . learn = cnn_learner(dls, resnet34, pretrained=True, metrics=accuracy).to_fp16() learn.fit_one_cycle(5) . epoch train_loss valid_loss accuracy time . 0 | 1.255100 | 1.078973 | 0.546559 | 00:06 | . 1 | 1.164489 | 1.024763 | 0.562753 | 00:05 | . 2 | 1.068163 | 0.723718 | 0.684211 | 00:06 | . 3 | 1.019540 | 0.661954 | 0.700405 | 00:06 | . 4 | 0.953290 | 0.638257 | 0.712551 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . The baseline model got and accuracy of ~61.27% on the test set. . learn.validate(ds_idx=2)[1] . 0.6126760840415955 . Cross Validation . from sklearn.model_selection import StratifiedKFold . train_labels = L() for i in range(len(dsrc.train)): train_labels.append(dsrc.train[i][1]) for i in range(len(dsrc.valid)): train_labels.append(dsrc.valid[i][1]) train_labels . (#1239) [TensorCategory(1),TensorCategory(1),TensorCategory(1),TensorCategory(0),TensorCategory(0),TensorCategory(0),TensorCategory(1),TensorCategory(0),TensorCategory(1),TensorCategory(1)...] . random.shuffle(train_imgs) . Our training loop will include the same process we&#39;ve done above on the DDSM Mass Dataset and splits section. That is, we will be splitting the training dataset 10 times and training a model on each of these datasets, after the training process is done we take the average of all the predictions for the test set on each split. . val_pct = [] tst_preds = [] skf = StratifiedKFold(n_splits=10, shuffle=True) for _, val_idx in skf.split(np.array(train_imgs), train_labels): splits = IndexSplitter(val_idx) split = splits(train_imgs) split_list = [split[0], split[1]] split_list.append(L(range(len(train_imgs), len(train_imgs)+len(tst_imgs)))) dsrc = Datasets(train_imgs+tst_imgs, tfms=[[PILImage.create], [parent_label, Categorize]], splits=split_list) dls = dsrc.dataloaders(bs=bs, after_item=item_tfms, after_batch=batch_tfms) learn = cnn_learner(dls, resnet34, pretrained=True, metrics=accuracy).to_fp16() learn.fit_one_cycle(5) val_pct.append(learn.validate()[1]) a,b = learn.get_preds(ds_idx=2) tst_preds.append(a) . . epoch train_loss valid_loss accuracy time . 0 | 1.136114 | 0.953437 | 0.491935 | 00:06 | . 1 | 1.066459 | 1.036575 | 0.548387 | 00:05 | . 2 | 1.011496 | 0.784486 | 0.669355 | 00:05 | . 3 | 0.968455 | 0.714560 | 0.669355 | 00:05 | . 4 | 0.928864 | 0.756449 | 0.653226 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . epoch train_loss valid_loss accuracy time . 0 | 1.343768 | 0.991764 | 0.572581 | 00:05 | . 1 | 1.203773 | 0.972596 | 0.629032 | 00:05 | . 2 | 1.097104 | 0.879680 | 0.669355 | 00:06 | . 3 | 1.015946 | 0.630741 | 0.725806 | 00:06 | . 4 | 0.939799 | 0.595138 | 0.733871 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.401825 | 0.975166 | 0.564516 | 00:05 | . 1 | 1.211842 | 1.150666 | 0.620968 | 00:05 | . 2 | 1.106027 | 0.824127 | 0.693548 | 00:05 | . 3 | 1.005552 | 0.643290 | 0.717742 | 00:05 | . 4 | 0.949984 | 0.611353 | 0.685484 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.285932 | 1.116190 | 0.516129 | 00:05 | . 1 | 1.176994 | 0.952720 | 0.629032 | 00:05 | . 2 | 1.059058 | 0.891377 | 0.637097 | 00:06 | . 3 | 0.987869 | 0.730751 | 0.701613 | 00:06 | . 4 | 0.942768 | 0.642452 | 0.701613 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.271043 | 0.824401 | 0.540323 | 00:05 | . 1 | 1.188266 | 1.026128 | 0.564516 | 00:05 | . 2 | 1.083751 | 0.844839 | 0.596774 | 00:05 | . 3 | 1.015182 | 0.688445 | 0.669355 | 00:06 | . 4 | 0.949915 | 0.696868 | 0.677419 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.252791 | 0.952793 | 0.475806 | 00:05 | . 1 | 1.091202 | 0.878849 | 0.596774 | 00:05 | . 2 | 1.030211 | 0.801414 | 0.661290 | 00:06 | . 3 | 0.971240 | 0.716754 | 0.693548 | 00:05 | . 4 | 0.923688 | 0.656634 | 0.717742 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.171420 | 0.854203 | 0.548387 | 00:05 | . 1 | 1.043760 | 0.852307 | 0.637097 | 00:06 | . 2 | 0.946355 | 0.857245 | 0.604839 | 00:05 | . 3 | 0.898549 | 0.815291 | 0.645161 | 00:05 | . 4 | 0.841311 | 0.815915 | 0.677419 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.348083 | 0.972769 | 0.564516 | 00:05 | . 1 | 1.179842 | 0.945408 | 0.620968 | 00:05 | . 2 | 1.112100 | 0.715282 | 0.653226 | 00:05 | . 3 | 1.044266 | 0.638139 | 0.717742 | 00:05 | . 4 | 0.984065 | 0.619281 | 0.717742 | 00:06 | . epoch train_loss valid_loss accuracy time . 0 | 1.230497 | 0.981037 | 0.500000 | 00:06 | . 1 | 1.114652 | 0.823075 | 0.653226 | 00:06 | . 2 | 1.009943 | 0.715533 | 0.653226 | 00:06 | . 3 | 0.941501 | 0.730544 | 0.629032 | 00:06 | . 4 | 0.899372 | 0.703373 | 0.629032 | 00:06 | . epoch train_loss valid_loss accuracy time . 0 | 1.403852 | 0.920186 | 0.552846 | 00:06 | . 1 | 1.189877 | 0.931247 | 0.658537 | 00:05 | . 2 | 1.064095 | 0.824453 | 0.658537 | 00:05 | . 3 | 1.018285 | 0.733249 | 0.642276 | 00:05 | . 4 | 0.971781 | 0.751345 | 0.650406 | 00:05 | . Now how do we combine all our predictions? We sum them all together then divide by our total . tst_preds_copy = tst_preds.copy() . accuracy(tst_preds_copy[0], b) . tensor(0.6268) . Now let&#39;s add them all together and get an average prediction . for i in tst_preds_copy: print(accuracy(i, b)) . tensor(0.6479) tensor(0.5986) tensor(0.6268) tensor(0.6127) tensor(0.6408) tensor(0.6127) tensor(0.6549) tensor(0.6338) tensor(0.6831) tensor(0.6408) . hat = tst_preds[0] for pred in tst_preds[1:]: hat += pred . hat[:5] . tensor([[3.7651, 6.2349], [9.1702, 0.8298], [8.2053, 1.7947], [9.8177, 0.1823], [4.1707, 5.8293]]) . len(hat) . 142 . hat /= len(tst_preds) . This accuracy is the average of all the predictions of the models trained on the splits. ~~65.49% . accuracy(hat, b) . tensor(0.6549) . This is the process we&#39;ll be using to conduct our experiments on several CNN models. .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastpages/jupyter/fastai/ddsm/mass/2020/04/06/CrossValidation.html",
            "relUrl": "/fastpages/jupyter/fastai/ddsm/mass/2020/04/06/CrossValidation.html",
            "date": " • Apr 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jmtzt.github.io/jmtzt_notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jmtzt.github.io/jmtzt_notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}