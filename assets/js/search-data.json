{
  
    
        "post0": {
            "title": "Experiments with SoA techniques",
            "content": "Imports . from fastai2.vision.all import * from utils import * path = Path(&#39;./DDSM_NOBARS/MASS/&#39;); path.ls() . (#2) [Path(&#39;DDSM_NOBARS/MASS/benigna&#39;),Path(&#39;DDSM_NOBARS/MASS/maligna&#39;)] . DataBlock/DataLoaders . dblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(seed=42), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) #dblock.summary(path) dls = dblock.dataloaders(path, bs=64) . Baseline run . model = resnet34() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate]).to_fp16() learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 7.298484 | 49.292488 | 0.000000 | 1.000000 | 00:06 | . 1 | 3.581588 | 1.451827 | 0.404858 | 0.595142 | 00:06 | . 2 | 2.321033 | 0.743234 | 0.461538 | 0.538462 | 00:06 | . 3 | 1.713093 | 0.695602 | 0.502024 | 0.497976 | 00:06 | . 4 | 1.362898 | 0.681767 | 0.595142 | 0.404858 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Normalization . Testing if the data is normalized i.e. has a mean of 0 and a std of 1. . x,y = dls.one_batch() x.shape,y.shape . (torch.Size([64, 3, 224, 224]), torch.Size([64])) . x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.5666, 0.5666, 0.5666], device=&#39;cuda:0&#39;), TensorImage([0.1686, 0.1686, 0.1686], device=&#39;cuda:0&#39;)) . The mean and standart deviation are not close to the desired values, therefore we need to normalize them by adding to the DataBlock the Normalize transform that uses the ImageNet mean and std. . def get_dls(bs, size): dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(seed=42), item_tfms=Resize(460), batch_tfms=[*aug_transforms(size=size, min_scale=0.75), Normalize.from_stats(*imagenet_stats)]) return dblock.dataloaders(path, bs=bs) . dls = get_dls(64, 224) x,y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.3459, 0.4831, 0.7032], device=&#39;cuda:0&#39;), TensorImage([0.7476, 0.7643, 0.7609], device=&#39;cuda:0&#39;)) . Checking if it affects our model . model = resnet34() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate]).to_fp16() learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 6.589247 | nan | 0.534413 | 0.465587 | 00:06 | . 1 | 3.267678 | 1.586851 | 0.615385 | 0.384615 | 00:06 | . 2 | 2.144929 | 1.013776 | 0.599190 | 0.400810 | 00:06 | . 3 | 1.608513 | 0.705930 | 0.437247 | 0.562753 | 00:06 | . 4 | 1.298861 | 0.669487 | 0.619433 | 0.380567 | 00:06 | . We can see some improvement in our acccuracy, but not a whole lot, because we are training this model from scratch. If we were to train this model based on a transfer learning approach, we would have to pay even more attention to that, in order to match the statistics used for normalization of the pre-trained model. . Progressive Resizing . This approach is basically gradually using larger and larger images as you train your model . dls = get_dls(128, 128) learn = Learner(dls, resnet34(), loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate]).to_fp16() learn.fit_one_cycle(4, 3e-3) . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . epoch train_loss valid_loss accuracy error_rate time . 0 | 7.789251 | 7.220915 | 0.000000 | 1.000000 | 00:05 | . 1 | 6.119124 | nan | 0.570850 | 0.429150 | 00:05 | . 2 | 4.074199 | nan | 0.562753 | 0.437247 | 00:05 | . 3 | 3.038997 | 5.341589 | 0.518219 | 0.481781 | 00:06 | . We increase the image resolution and decrease the batch size and proceed to fine-tune the model: . learn.dls = get_dls(64, 224) learn.fine_tune(5, 1e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 0.699829 | 0.706627 | 0.510121 | 0.489879 | 00:06 | . epoch train_loss valid_loss accuracy error_rate time . 0 | 0.677542 | 0.713452 | 0.599190 | 0.400810 | 00:06 | . 1 | 0.681381 | 0.686060 | 0.635628 | 0.364372 | 00:06 | . 2 | 0.682831 | 0.699931 | 0.510121 | 0.489879 | 00:06 | . 3 | 0.685403 | 0.668482 | 0.619433 | 0.380567 | 00:06 | . 4 | 0.682514 | 0.671242 | 0.663968 | 0.336032 | 00:06 | . As we can see, the accuracy improved when using this technique. . Test Time Augmentation (TTA) . Up until now we have been using random cropping as data augmentation, which may lead to some problems such as some critical features being cropped out of the image. One technique that might help mitigate this problem is select a number of areas to crop from the original rectangular image and then pass each of them through the model and take the average of the predictions, that is just applying a form of augmentation in the validation dataset as well. . preds,targs = learn.tta() accuracy(preds, targs).item() . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . 0.659919023513794 . Mixup Technique . mixup: Beyond Empirical Risk Minimization: https://arxiv.org/abs/1710.09412 . Mixup works as follows, for each image: . Select another image from your dataset at random. | Pick a weight at random. | Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable. | Take a weighted average (with the same weight) of this image&#39;s labels with your image&#39;s labels; this will be your dependent variable. | ben = PILImage.create(get_image_files_sorted(path/&#39;benigna&#39;)[0]) mal = PILImage.create(get_image_files_sorted(path/&#39;maligna&#39;)[0]) ben = ben.resize((256,256)) mal = mal.resize((256,256)) tben = tensor(ben).float() / 255. tmal = tensor(mal).float() / 255. _,axs = plt.subplots(1, 3, figsize=(12,4)) show_image(tben, ax=axs[0]); show_image(tmal, ax=axs[1]); show_image((0.3*tben + 0.7*tmal), ax=axs[2]); . The third image is 30% the first one and 70% the second one, so the model must predict 30% benign and 70% malign. So the one-hot-encoded representations of the predicitons are (in this dataset which we have 2 classes): . [1,0] and [0, 1] . But we are aiming for this type of prediction: . [0.3, 0.7] . model = resnet34() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, error_rate], cbs=MixUp) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 4.664900 | 208.644165 | 0.591093 | 0.408907 | 00:07 | . 1 | 2.637848 | 194.706757 | 0.591093 | 0.408907 | 00:06 | . 2 | 1.903277 | 1.384996 | 0.526316 | 0.473684 | 00:06 | . 3 | 1.535061 | 0.859590 | 0.534413 | 0.465587 | 00:06 | . 4 | 1.315490 | 0.679021 | 0.615385 | 0.384615 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Training in this way, without explicitly telling the model that the labels must be biggen than 0 but smaller than 1, makes our activations more extreme as we train for more epochs. That is the reason why we will use label smoothing to deal with this. . Label Smoothing . Rethinking the Inception Architecture for Computer Vision: https://arxiv.org/abs/1512.00567 . Instead of using regular one-hot-encoded vectors for the targets, we should use targets in the following format: . [0.1, 0.9] . This helps we do not encourage the model to predict something overconfidently. . model = resnet34() learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=[accuracy, error_rate], cbs=MixUp) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 4.738817 | 4001.799072 | 0.000000 | 1.000000 | 00:07 | . 1 | 3.226263 | 1226.570801 | 0.591093 | 0.408907 | 00:07 | . 2 | 2.642515 | 60.685635 | 0.518219 | 0.481781 | 00:06 | . 3 | 2.353480 | 4.580351 | 0.554656 | 0.445344 | 00:06 | . 4 | 2.170954 | 1.858582 | 0.530364 | 0.469636 | 00:07 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Normally we see improvements both from the MixUp and the Label Smoothing technique when we train the model for more epochs. .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastpages/jupyter/fastai/ddsm/mass/2020/05/30/DDSM-Experiments.html",
            "relUrl": "/fastpages/jupyter/fastai/ddsm/mass/2020/05/30/DDSM-Experiments.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Cross-Validation with fastai v2",
            "content": "from fastai2.vision.all import * path = Path(&#39;./DDSM_NOBARS/&#39;); path.ls() . DDSM Mass Dataset and splits . batch_tfms = [IntToFloatTensor(), *aug_transforms(size=224, max_warp=0, min_scale=0.75), Normalize.from_stats(*imagenet_stats)] item_tfms = [ToTensor(), Resize(460)] bs=64 . train_imgs = get_image_files(path/&#39;train&#39;) tst_imgs = get_image_files(path/&#39;test&#39;) . random.shuffle(train_imgs) . len(train_imgs) . 1239 . Here we do an 80/20 split . start_val = len(train_imgs) - int(len(train_imgs)*.2) # last 20% validation . idxs = list(range(start_val, len(train_imgs))) . splits = IndexSplitter(idxs) . split = splits(train_imgs) . len(train_imgs) . 1239 . split_list = [split[0], split[1]] . split_list.append(L(range(len(train_imgs), len(train_imgs)+len(tst_imgs)))) . We have 992 training images, 247 validation images and 142 test images . split_list . [(#992) [0,1,2,3,4,5,6,7,8,9...], (#247) [992,993,994,995,996,997,998,999,1000,1001...], (#142) [1239,1240,1241,1242,1243,1244,1245,1246,1247,1248...]] . dsrc = Datasets(train_imgs+tst_imgs, tfms=[[PILImage.create], [parent_label, Categorize]], splits = split_list) . show_at(dsrc.train, 3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8fa00900d0&gt; . dsrc.n_subsets . 3 . dls = dsrc.dataloaders(bs=bs, after_item=item_tfms, after_batch=batch_tfms) . dls.show_batch() . Baseline . learn = cnn_learner(dls, resnet34, pretrained=True, metrics=accuracy).to_fp16() learn.fit_one_cycle(5) . epoch train_loss valid_loss accuracy time . 0 | 1.255100 | 1.078973 | 0.546559 | 00:06 | . 1 | 1.164489 | 1.024763 | 0.562753 | 00:05 | . 2 | 1.068163 | 0.723718 | 0.684211 | 00:06 | . 3 | 1.019540 | 0.661954 | 0.700405 | 00:06 | . 4 | 0.953290 | 0.638257 | 0.712551 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . The baseline model got and accuracy of ~61.27% on the test set. . learn.validate(ds_idx=2)[1] . 0.6126760840415955 . Cross Validation . from sklearn.model_selection import StratifiedKFold . train_labels = L() for i in range(len(dsrc.train)): train_labels.append(dsrc.train[i][1]) for i in range(len(dsrc.valid)): train_labels.append(dsrc.valid[i][1]) train_labels . (#1239) [TensorCategory(1),TensorCategory(1),TensorCategory(1),TensorCategory(0),TensorCategory(0),TensorCategory(0),TensorCategory(1),TensorCategory(0),TensorCategory(1),TensorCategory(1)...] . random.shuffle(train_imgs) . Our training loop will include the same process we&#39;ve done above on the DDSM Mass Dataset and splits section. That is, we will be splitting the training dataset 10 times and training a model on each of these datasets, after the training process is done we take the average of all the predictions for the test set on each split. . val_pct = [] tst_preds = [] skf = StratifiedKFold(n_splits=10, shuffle=True) for _, val_idx in skf.split(np.array(train_imgs), train_labels): splits = IndexSplitter(val_idx) split = splits(train_imgs) split_list = [split[0], split[1]] split_list.append(L(range(len(train_imgs), len(train_imgs)+len(tst_imgs)))) dsrc = Datasets(train_imgs+tst_imgs, tfms=[[PILImage.create], [parent_label, Categorize]], splits=split_list) dls = dsrc.dataloaders(bs=bs, after_item=item_tfms, after_batch=batch_tfms) learn = cnn_learner(dls, resnet34, pretrained=True, metrics=accuracy).to_fp16() learn.fit_one_cycle(5) val_pct.append(learn.validate()[1]) a,b = learn.get_preds(ds_idx=2) tst_preds.append(a) . . epoch train_loss valid_loss accuracy time . 0 | 1.136114 | 0.953437 | 0.491935 | 00:06 | . 1 | 1.066459 | 1.036575 | 0.548387 | 00:05 | . 2 | 1.011496 | 0.784486 | 0.669355 | 00:05 | . 3 | 0.968455 | 0.714560 | 0.669355 | 00:05 | . 4 | 0.928864 | 0.756449 | 0.653226 | 00:06 | . /home/jmtzt/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . epoch train_loss valid_loss accuracy time . 0 | 1.343768 | 0.991764 | 0.572581 | 00:05 | . 1 | 1.203773 | 0.972596 | 0.629032 | 00:05 | . 2 | 1.097104 | 0.879680 | 0.669355 | 00:06 | . 3 | 1.015946 | 0.630741 | 0.725806 | 00:06 | . 4 | 0.939799 | 0.595138 | 0.733871 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.401825 | 0.975166 | 0.564516 | 00:05 | . 1 | 1.211842 | 1.150666 | 0.620968 | 00:05 | . 2 | 1.106027 | 0.824127 | 0.693548 | 00:05 | . 3 | 1.005552 | 0.643290 | 0.717742 | 00:05 | . 4 | 0.949984 | 0.611353 | 0.685484 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.285932 | 1.116190 | 0.516129 | 00:05 | . 1 | 1.176994 | 0.952720 | 0.629032 | 00:05 | . 2 | 1.059058 | 0.891377 | 0.637097 | 00:06 | . 3 | 0.987869 | 0.730751 | 0.701613 | 00:06 | . 4 | 0.942768 | 0.642452 | 0.701613 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.271043 | 0.824401 | 0.540323 | 00:05 | . 1 | 1.188266 | 1.026128 | 0.564516 | 00:05 | . 2 | 1.083751 | 0.844839 | 0.596774 | 00:05 | . 3 | 1.015182 | 0.688445 | 0.669355 | 00:06 | . 4 | 0.949915 | 0.696868 | 0.677419 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.252791 | 0.952793 | 0.475806 | 00:05 | . 1 | 1.091202 | 0.878849 | 0.596774 | 00:05 | . 2 | 1.030211 | 0.801414 | 0.661290 | 00:06 | . 3 | 0.971240 | 0.716754 | 0.693548 | 00:05 | . 4 | 0.923688 | 0.656634 | 0.717742 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.171420 | 0.854203 | 0.548387 | 00:05 | . 1 | 1.043760 | 0.852307 | 0.637097 | 00:06 | . 2 | 0.946355 | 0.857245 | 0.604839 | 00:05 | . 3 | 0.898549 | 0.815291 | 0.645161 | 00:05 | . 4 | 0.841311 | 0.815915 | 0.677419 | 00:05 | . epoch train_loss valid_loss accuracy time . 0 | 1.348083 | 0.972769 | 0.564516 | 00:05 | . 1 | 1.179842 | 0.945408 | 0.620968 | 00:05 | . 2 | 1.112100 | 0.715282 | 0.653226 | 00:05 | . 3 | 1.044266 | 0.638139 | 0.717742 | 00:05 | . 4 | 0.984065 | 0.619281 | 0.717742 | 00:06 | . epoch train_loss valid_loss accuracy time . 0 | 1.230497 | 0.981037 | 0.500000 | 00:06 | . 1 | 1.114652 | 0.823075 | 0.653226 | 00:06 | . 2 | 1.009943 | 0.715533 | 0.653226 | 00:06 | . 3 | 0.941501 | 0.730544 | 0.629032 | 00:06 | . 4 | 0.899372 | 0.703373 | 0.629032 | 00:06 | . epoch train_loss valid_loss accuracy time . 0 | 1.403852 | 0.920186 | 0.552846 | 00:06 | . 1 | 1.189877 | 0.931247 | 0.658537 | 00:05 | . 2 | 1.064095 | 0.824453 | 0.658537 | 00:05 | . 3 | 1.018285 | 0.733249 | 0.642276 | 00:05 | . 4 | 0.971781 | 0.751345 | 0.650406 | 00:05 | . Now how do we combine all our predictions? We sum them all together then divide by our total . tst_preds_copy = tst_preds.copy() . accuracy(tst_preds_copy[0], b) . tensor(0.6268) . Now let&#39;s add them all together and get an average prediction . for i in tst_preds_copy: print(accuracy(i, b)) . tensor(0.6479) tensor(0.5986) tensor(0.6268) tensor(0.6127) tensor(0.6408) tensor(0.6127) tensor(0.6549) tensor(0.6338) tensor(0.6831) tensor(0.6408) . hat = tst_preds[0] for pred in tst_preds[1:]: hat += pred . hat[:5] . tensor([[3.7651, 6.2349], [9.1702, 0.8298], [8.2053, 1.7947], [9.8177, 0.1823], [4.1707, 5.8293]]) . len(hat) . 142 . hat /= len(tst_preds) . This accuracy is the average of all the predictions of the models trained on the splits. ~~65.49% . accuracy(hat, b) . tensor(0.6549) . This is the process we&#39;ll be using to conduct our experiments on several CNN models. .",
            "url": "https://jmtzt.github.io/jmtzt_notes/fastpages/jupyter/fastai/ddsm/mass/2020/04/06/CrossValidation.html",
            "relUrl": "/fastpages/jupyter/fastai/ddsm/mass/2020/04/06/CrossValidation.html",
            "date": " • Apr 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jmtzt.github.io/jmtzt_notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jmtzt.github.io/jmtzt_notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}